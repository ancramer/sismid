---
title: "Forecast evaluation"
order: 9
bibliography: ../nfidd.bib
---

# Introduction

So far we have focused on building and visualisation simple forecasts.
In this session you will get to know several ways of assessing different aspects of forecast performance.
One of these approaches involves visualising forecasts in the context of eventually observed data.
Another approach involves summarising performance quantitatively.

## Slides

- [Forecast evaluation](slides/forecast-evaluation)

## Objectives

The aim of this session is to introduce the concept of forecast evaluation from a qualitative and quantitative perspective.

::: {.callout-note collapse="true"}

# Setup

## Source file

The source file of this session is located at `sessions/forecasting-evaluation.qmd`.

## Libraries used

In this session we will use the `fable` package for fitting and evaluating simple forecasting models, the `dplyr` package for data wrangling, the `ggplot2` library for plotting and the `epidatr` package for accessing and downloading versions of epidemiological surveillance data from the [Delphi EpiData API](https://cmu-delphi.github.io/delphi-epidata/).

```{r libraries, message = FALSE}
library("fable")
library("dplyr")
library("ggplot2")
library("epidatr")
theme_set(theme_bw())
```

::: {.callout-tip}
The best way to interact with the material is via the [Visual Editor](https://docs.posit.co/ide/user/ide/guide/documents/visual-editor.html) of RStudio.
:::

## Initialisation

We set a random seed for reproducibility. 
Setting this ensures that you should get exactly the same results on your computer as we do.
We also set an option that makes `cmdstanr` show line numbers when printing model code.
This is not strictly necessary but will help us talk about the models.

```{r}
set.seed(42) # for Jackie Robinson!
```

:::

# Introduction to forecast evaluation

An important aspect of making forecasts is that we can later confront the forecasts with what really happened and use this to assess whether our forecast model makes good predictions, or which of multiple models work best in which situation.
Different approaches may be appropriate for point forecasts and probabilistic forecasts. 
We will mostly focus on probabilistic forecasts, as we tend to think that providing probabilities can assist with more nuanced decision-making strategies.

::: {.callout-tip}
## What do we look for in a good probabilistic forecast? Some food for thought:
1. **Calibration**: The forecast should be well calibrated. This means that the forecasted probabilities should match the observed frequencies. For example, if the model predicts a 50% probability of an event occurring, then the event should occur approximately 50% of the time.
2. **Unbiasedness**: The forecast should be unbiased. This means that the average forecasted value should be equal to the average observed value. It shouldn't consistently over- or under-predict.
3. **Accuracy**: The forecast should be accurate. This means that the forecasted values should be close to the observed values.
4. **Sharpness**: As long as the other conditions are fulfilled we want prediction intervals to be as narrow as possible. Predicting that "anything can happen" might be correct but not very useful.

Note that sharpness is a property of the forecasts themselves, whereas calibration, unbiasedness, and accuracy are properties that emerge from comparing forecasts to observed data.

## The forecasting paradigm

The general principle underlying forecast evaluation is to **maximise *sharpness* subject to *calibration*** [@gneiting2007].
This means that statements about the future should be **correct** (calibration) and should aim to have **narrow uncertainty** (sharpness).
:::


# Challenges in forecast evaluation

In order to rigorously evaluate forecasts from a model, there will often be multiple dimensions at play.
For example, you might run a forecasting experiment where you make forecasts ...

- at multiple different times, based on the data available as of that time
- for multiple different locations
- for a number of different horizons into the future, (e.g. 1, 2 and 3 weeks into the future, or 1 through 40 days into the future.)
- with several or many different modeling approaches

In epidemiological forecasting, it is common to have projects where all four of these dimensions are in play.

Keeping track of these dimensions can be challenging. 
It also gives us a lot of model output data to work with when using scoring metrics. 
And raises challenges about doing statistically rigorous "testing" when trying to compare average errors across differnet dimensions.


# Visual forecast evaluation

We will now generate some forecasts from one of the models we used earlier and evaluate them.
However, to set us up for a more multi-dimensional experiment later in this session, we will start by loading in a bit more data than we did before.

```{r load_data}
flu_data <- epidatr::pub_fluview(regions = c("nat"),
                                 epiweeks = epirange(200335, 201825)) |> 
  select(region, epiweek, wili) |> 
  as_tsibble(index = epiweek, key = region)
```

In the above code, we've pulled data for the 2017/2018 season (previously we had only pulled data up to the start of the 2017/2018 season).

Below, we will fit and forecast from the ARIMA model plus Fourier terms that we used in the last session. 
Note that we filter the data to start at the beginning of the season!
This way, the model can't see the data from later in the season when making the prediction (although we still could look at it).

```{r simple-forecasts}
## remember to define the transformation
fourth_root <- function(x) x^0.25
inv_fourth_root <- function(x) x^4
my_fourth_root <- new_transformation(fourth_root, inv_fourth_root)

## here is the model
fit_fourier_ar2 <- flu_data |>
  filter(epiweek <= "2017-08-27") |> 
  model(
    arima200_fourier = ARIMA(my_fourth_root(wili) ~ pdq(2,0,0) + fourier(period = "year", K=3))
    )

## make the forecast
forecast_fourier_ar2 <- forecast(fit_fourier_ar2, h=40) 

## plot the forecast against all of the data from the 2017/2018 season
forecast_fourier_ar2 |>
  autoplot(flu_data |> filter(epiweek >= as.Date("2015-09-01"))) +
  facet_wrap(~.model) +
  labs(title = "WILI, US level",
       y="% of visits")
```

For the first time in the forecasting part of this course, we have confronted a model with the eventually observed data.
And... well, what do you think?

## Take 5 minutes

Look at the plot of data and forecasts carefully.
Write down at least two observations about the forecasts and how well they seem to match the observed data.
Share notes with your neighbor.

::: {.callout-note collapse="true"}
## Solution
Once again, there isn't an exact correct answer here, but here are a few observations.

- The model seems to do fairly well at the tails of the season.
- The model's predictions for the epidemiologically important part (the peak!) is not very good - it's much too low. The observed data are outside the 95% PI intervals for many weeks in a row.

But, maybe this isn't the end of the world. It is a lot, afterall, to ask a model to predict several months into the future. 
(We don't ask weather models to do that, generally. Or if we do, we probably don't trust them that much.)
:::


# Quantitative forecast evaluation

We now summarise performance quantitatively by using scoring metrics.
Whilst some of these metrics are more useful for comparing models, many can be also be useful for understanding the performance of a single model.

::: {.callout-tip}
In this session, we'll mostly be using "proper" scoring rules: these are scoring rules for probabilistic forecasts that make sure no model can get better scores than the *true* model, i.e. the model used to generate the data.
Of course we usually don't know this (as we don't know the "true model" for real-world data) but proper scoring rules incentivise forecasters to make their best attempt at reproducing its behaviour.
For a comprehensive (and fairly technical) text on proper scoring rules and their mathematical properties, we recommend the classic paper by @gneiting2007.
:::

We start by computing some summary metrics across all of the forecasted horizons. 
We will initially compute three metrics (math details to follow):

- mean absolute error (MAE)
- root mean squared error (RMSE)
- continuous ranked probability score (CRPS)

```{r}
accuracy(forecast_fourier_ar2, 
         data = flu_data, 
         measures = list(mae = MAE, rmse = RMSE, crps=CRPS))
```


The results shown above are "Test" results, meaning they are metrics computed by comparing the forecasts to the eventually observed data.
This is compared to "Training" results, which is the metrics computed "in-sample", that is, to the data the model was allowed to see when fitting.
You can see the training set results (for point-prediction metrics, at least) by passing the model fit object to the `accuracy()` function:

```{r}
accuracy(fit_fourier_ar2, measures = list(mae = MAE, rmse = RMSE))
```

::: {.callout-tip}
Because of the flavor of object-oriented programming used in `fable`, it can be tricky to figure out how to find the help pages for functions. 
For example, typing the standard `?accuracy` does not get you to a useful page. 
Instead, you need to type `?accuracy.fbl_ts`, since the `forecast_fourier_ar2` is a an object with class `"fbl_ts"`.
:::

## Mean absolute error

### What is the Mean Absolute Error (MAE)?

For point forecasts (single value predictions), forecast accuracy is commonly measured using the Mean Absolute Error (MAE), which calculates the average (mean) absolute difference between predicted and observed values. 
Mathematically, this can be expressed simply as 
$$
MAE(\hat y, y) = \frac{1}{T} \sum_{t=1}^T |\hat y_t - y_t|
$$
where $y_t$ is the observed value at time $t$, $\hat y_t$ is the point prediction, and $T$ is the total number of predictions.

::: {.callout-tip}
### Key things to note about the MAE
  - Small values are better.
  - As it is an absolute scoring rule it can be difficult to use to compare forecasts where the observed values are on different scales. For example, if you are comparing forecasts for counts in location A, where the counts tend to be over ten thousand, to counts from location B, where counts tend to be under 100.
  - The metric is on the scale of the data. The number MAE returns can be thought of as the average absolute distance between your prediction and the eventually observed value.
:::

## Mean squared error

### What is the Root Mean Squared Error (RMSE)?

Another common point forecast accuracy measure is the Root Mean Squared Error (MSE), which calculates the square root of the average (mean) squared difference between predicted and observed values. 
Mathematically, this can be expressed simply as 
$$
RMSE(\hat y, y) = \sqrt{\frac{1}{T} \sum_{t=1}^T (\hat y_t - y_t)^2}
$$
where $y_t$ is the observed value at time $t$, $\hat y_t$ is the point prediction, and $T$ is the total number of predictions.

::: {.callout-tip}
### Key things to note about the RMSE
  - Small values are better.
  - As it is an absolute scoring rule it can be difficult to use to compare forecasts where the observed values are on different scales (same as MAE).
  - It tends to penalize forecasts that have large misses, because those distances will be squared and have an outsize impact on the mean.
  - The metric is on the scale of the data. 
:::


## Continuous ranked probability score

### What is the Continuous Ranked Probability Score (CRPS)?

For probabilistic forecasts, where the forecast is a distribution rather than a single point estimate, we can use the Continuous Ranked Probability Score (CRPS). 
The CRPS is a proper scoring rule that generalises MAE to probabilistic forecasts. 
Note that for deterministic forecasts, CRPS reduces to MAE.

The CRPS can be thought about as the combination of two key aspects of forecasting:
1. The accuracy of the forecast in terms of how close the predicted values are to the observed value.
2. The confidence of the forecast in terms of the spread of the predicted values.

By balancing these two aspects, the CRPS provides a comprehensive measure of the quality of probabilistic forecasts.

::: {.callout-tip}
### Key things to note about the CRPS
  - Small values are better
  - As it is an absolute scoring rule it can be difficult to use to compare forecasts across scales.
  - The metric is on the scale of the data. It can be thought of, in a heuristic kind of way, as a measure of distance between the predicted distribution and the observed value.

:::


::: {.callout-tip collapse="true"}
### Mathematical Definition (optional)

The CRPS for a predictive distribution characterised by a cumulative distribution function $F$ and observed value $x$ is calculated as

$$
CRPS(F, y) = \int_{-\infty}^{+\infty} \left( F(x) - \mathbb{1} ({x \geq y})^2 \right) dx.
$$

For distributions with a finite first moment (a mean exists and it is finite), the CRPS can be expressed as:

$$
CRPS(F, y) = \mathbb{E}_{X \sim F}[|X - y|] - \frac{1}{2} \mathbb{E}_{X, X' \sim F}[|X - X'|]
$$

where $X$ and $X'$ are independent random variables sampled from the distribution $F$. 
To calculate this we simply replace $X$ and $X'$ by samples from our posterior distribution and sum over all possible combinations.

:::

## Looking at the scores by horizon

Whilst the metrics described above are very useful metrics they can be difficult to interpret in isolation. 
For example, it is often useful to compare the CRPS of different models or to compare the CRPS of the same model under different conditions. 
Let's compare the all of the metrics across different forecast horizons.

```{r}
metrics_fourier_ar2 <- forecast_fourier_ar2 |> 
  mutate(h = row_number()) |> 
  accuracy(data = flu_data, 
           measures = list(mae = MAE, rmse = RMSE, crps=CRPS),
           by = c(".model", "h"))
```

And we could plot these metrics by horizon.
```{r}
metrics_fourier_ar2 |> 
  tidyr::pivot_longer(
    cols = c("mae", "rmse", "crps"),
    names_to = "metric"
    ) |> 
  ggplot() +
  geom_line(aes(x = h, y = value, color = metric)) +
  facet_wrap(.~metric)
  
```


::: {.callout-tip}
## Take 5 minutes 
How do the scores change with forecast horizon?
How similar to or different from each other are the metrics?
What do you think the forecast error trends by horizon would look like if you made one forecast in the middle of the season?
Which of the additional dimensions mentioned at the top of the lesson would you like to examine more closely for forecast accuracy?
:::

::: {.callout-note collapse="true"}
## Solution
  - All of the metrics increase for horizons where true incidence is higher. 
  - The forecast errors are strongly associated with the seasonality of the data.
:::


# Comparing three models at multiple time-points

```{r}
## set up time-series cross-validation with 732 initial observations and every 4 weeks
flu_data_tscv <- flu_data |> 
  tsibble::stretch_tsibble(
    .init = 732, 
    .step = 4
    )

cv_forecasts <- flu_data_tscv |> 
  model(
    ar2 = ARIMA(my_fourth_root(wili) ~ pdq(2,0,0)),
    fourier = ARIMA(my_fourth_root(wili) ~ fourier(period = "year", K=3)),
    fourier_ar2 = ARIMA(my_fourth_root(wili) ~ pdq(2,0,0) + fourier(period = "year", K=3))
  ) |> 
  forecast(h = 8) |> 
  group_by(.id, .model) |> 
  mutate(h = row_number()) |> 
  ungroup() |> 
  as_fable(response = "wili", distribution = wili)

cv_forecasts |> 
  accuracy(
    flu_data, 
    by = c("h", ".model"), 
    measures = list(mae = MAE, rmse = RMSE, crps=CRPS)
  ) |> 
  ggplot(aes(x = h, y = crps, color = .model)) +
  geom_point() +
  geom_line()

fit_fourier_only <- flu_data |>
  filter(epiweek <= "2017-08-27") |> 
  model(
    arima200_fourier = ARIMA(my_fourth_root(wili) ~ fourier(period = "year", K=3))
    )

## make the forecast
forecast_fourier_only <- forecast(fit_fourier_only, h=40) 

## plot the forecast against all of the data from the 2017/2018 season
forecast_fourier_only |>
  autoplot(flu_data |> filter(epiweek >= as.Date("2015-09-01"))) +
  facet_wrap(~.model) +
  labs(title = "WILI, US level",
       y="% of visits")
```



## Scoring on the log scale

?? should we include??

::: {.callout-tip}
## Take 5 minutes 
How do the CRPS scores change based on forecast date?
How do the CRPS scores change with forecast horizon?
What does this tell you about the model?
:::

::: {.callout-note collapse="true"}
## Solution
- As for the natural scale CRPS scores increase with forecast horizon but now the increase appears to be linear vs exponential.
- There has been a reduction in the CRPS scores for forecast dates near the outbreak peak compared to other forecast dates but this is still the period where the model is performing worst.
:::

# Going further

## Challenge

- In which other ways could we summarise the performance of the forecasts?
- What other metrics could we use?
- There is no one-size-fits-all approach to forecast evaluation, often you will need to use a combination of metrics to understand the performance of your model and typically the metrics you use will depend on the context of the forecast. 
What attributes of the forecast are most important to you?
- One useful way to think about evaluating forecasts is to consider exploring the scores as a data analysis in its own right. 
For example, you could look at how the scores change over time, how they change with different forecast horizons, or how they change with different models. 
This can be a useful way to understand the strengths and weaknesses of your model. 
Explore some of these aspects using the scores from this session.

## Methods in practice

- There are many other metrics that can be used to evaluate forecasts. 
The [documentation](https://epiforecasts.io/scoringutils/dev/articles/metric-details.html) for the `{scoringutils}` package has a good overview of these metrics and how to use them.

# Wrap up

- Review what you've learned in this session with the [learning objectives](../reference/learning_objectives)

::: {#refs}
:::

---
title: "Forecast ensembles"
order: 11
bibliography: ../nfidd.bib
---

# Introduction

As we saw in the sessions on [improving forecasting models](forecasting-models) and [forecast evaluation](forecasting-evaluation), different modelling approaches have different strength and weaknesses, and we do not usually know in advance which one will produce the best forecast in any given situation.
One way to attempt to draw strength from a diversity of approaches is the creation of so-called *forecast ensembles* from the forecasts produced by different models.

In this session, we'll start with forecasts from the models we explored in the previous sessions and build ensembles of these models.
We will then compare the performance of these ensembles to the individual models and to each other.

::: {.callout-note collapse="false"}
## Representations of probabilistic forecasts

Probabilistic predictions can be described as coming from a probabilistic probability distributions.
In general, it is not safe to assume that these distributions can be expressed in a simple mathematical form as we can do if, e.g., talking about common probability distributions such as the normal or gamma distributions.
(Although some of the `fable` models, like ARIMA can express forecasts as normal distributions.)

It can be helpful to have a model-agnostic way to represent probabilistic distributions.

-   One common way is to use a limited number (say, between 100 and 1000) of **samples** generated from Monte Carlo methods to represent the predictive distribution.

-   Another approach uses values that corresponds to a specific set of **quantile** levels of the predictive distribution distribution.
    For example, the median is the 50th quantile of a distribution, meaning that 50% of the values in the distribution are less than the median and 50% are greater.
    Similarly, the 90th quantile is the value that corresponds to 90% of the distribution being less than this value.
    If we characterise a predictive distribution by its quantiles, we specify these values at a range of specific quantile levels, e.g. from 5% to 95% in 5% steps.

-   Another approach might be to define **bins** (intervals) that an outcome might fall into and assign probabilities to those bins

Deciding how to represent forecasts depends on many things, for example the method used (and whether it produces samples by default) but also logistical considerations.
Many collaborative forecasting projects and so-called forecasting hubs use quantile-based representations of forecasts in the hope to be able to characterise both the centre and tails of the distributions more reliably and with less demand on storage space than a sample-based representation.
Quantiles are also better than a bin-based representation when the range of possible outcomes is large.
:::

## Slides

-   [Introduction to ensembles](slides/introduction-to-ensembles)

## Objectives

The aim of this session is to introduce the concept of ensembles of forecasts and to evaluate the performance of ensembles of the models we explored in the earlier sessions.

:::: {.callout-note collapse="true"}
# Setup

## Source file

The source file of this session is located at `sessions/forecast-ensembles.qmd`.

## Libraries used

In this session we will use the `fable` package for fitting simple forecasting models, the `dplyr` package for data wrangling, the `ggplot2` library for plotting and the `epidatr` package for accessing and downloading versions of epidemiological surveillance data from the [Delphi EpiData API](https://cmu-delphi.github.io/delphi-epidata/).

Additionally, we will use some [hubverse](https://hubverse.io/) packages such as `hubData` , `hubEvals`, `hubUtils`, `hubEnsembles` packages for building ensembles.

```{r libraries, message = FALSE}
library("fable")
library("dplyr")
library("ggplot2")
library("epidatr")
library("hubUtils")
library("hubEvals")
library("hubData")
library("hubEnsembles")
theme_set(theme_bw())
```

::: callout-tip
The best way to interact with the material is via the [Visual Editor](https://docs.posit.co/ide/user/ide/guide/documents/visual-editor.html) of RStudio.
:::

## Initialisation

We set a random seed for reproducibility.
Setting this ensures that you should get exactly the same results on your computer as we do.

```{r}
set.seed(42) ## for Douglas Adams!
```
::::

# Loading data

As we have done in previous sessions, we will load some ILI data using the `epidatr` package,

```{r load-epidata}
flu_data <- epidatr::pub_fluview(regions = c("nat"),
                                 epiweeks = epirange(200335, 201831)) |> 
  select(region, epiweek, wili) |> 
  as_tsibble(index = epiweek, key = region)
```

and define a fourth-root transformation for our forecast models

```{r fourth-root}
fourth_root <- function(x) x^0.25
inv_fourth_root <- function(x) x^4
my_fourth_root <- new_transformation(fourth_root, inv_fourth_root)
```

# Individual forecast models

In this session we will use the forecasts from the models we explored in the session on [forecast evaluation](forecast-evaluation).
These were:

-   A random walk model
-   An AR(2) autoregressive model
-   An AR(2) autoregressive model + Fourier terms for seasonality
-   Only Fourier terms for seasonality

As in the session on [forecast evaluation](forecast-evaluation), we will fit these models to a range of forecast dates.

```{r load_forecasts}
## make the time-series cross validation splits
flu_data_tscv <- flu_data |> 
  filter(epiweek <= as.Date("2018-07-01")) |> 
  tsibble::stretch_tsibble(
    .init = 732, 
    .step = 4,
    .id = ".split"
    )

## generate the forecasts
cv_models <- flu_data_tscv |> 
  model(
    rw = RW(my_fourth_root(wili)),
    ar2 = ARIMA(my_fourth_root(wili) ~ pdq(2,0,0)),
    fourier = ARIMA(my_fourth_root(wili) ~ pdq(0,0,0) + fourier(period = "year", K=3)),
    fourier_ar2 = ARIMA(my_fourth_root(wili) ~ pdq(2,0,0) + fourier(period = "year", K=3))
  ) 

cv_forecasts <- cv_models |> 
  forecast(h = 8) |> 
  ## the following 3 lines of code ensure that there is a horizon variable in the forecast data
  group_by(.split, .model) |> 
  mutate(h = row_number()) |> 
  ungroup() |> 
  ## this ensures that the output is a fable object
  as_fable(response = "wili", distribution = wili)
```

Plot all the forecasts

```{r}
autoplot(
  cv_forecasts,
  flu_data_tscv |> filter(epiweek >= as.Date("2017-07-01")),
  alpha = 0.5
) +
  facet_wrap(~.split) +
  theme(legend.position = "bottom")
```

Motivate pivot to hubverse-style data:

-   `fable` is good for simple models, but we want and need more complexity (e.g., the standard `fable` ensemble tool broke when trying to ensemble these 4 models together)
-   more generic format for model output coming from different places

## callout tip on what is the hubverse

Generate some samples for easier hubverse storage and conversion

```{r}
sampled_forecasts <- generate(
  cv_models, 
  h=8, 
  times=100) |> 
  group_by(.split, region, .model, .rep) |> 
  mutate(h = row_number())
```


# Forecast ensembles

We will now move to creating forecasts as a combination of multiple forecasts.
This procedure is also sometimes called *stacking*, and the resulting forecasts are said to come from *ensembles* of forecast models.

## Quantile-based ensembles

We will first consider forecasts based on the individual quantiles of each model.
This corresponds to a situation where each forecast aims to correctly determine a single target predictive distribution.
By taking an average of all models, we aim to get a better estimate of this distribution than from the individual models.
If we have reason to believe that some models are **consistently** better than others at estimating this distribution, we could try to create a weighted version of this average.

### Converting sample-based forecasts to quantile-based forecasts

In this session we will be thinking about forecasts in terms quantiles of the predictive distributions, we will need to convert our sample based forecasts to quantile-based forecasts.
We will do this by focusing on the *marginal distribution* at each predicted time point, that is we treat each time point as independent of all others and calculate quantiles based on the sample predictive trajectories at that time point.
An easy way to do this is to use the `hubUtils` package, which has utility functions for interacting with hubverse-style data.
The steps to do this are to first do some reformatting so the forecasts can be seen as hubverse-style `sample` forecasts.

```{r convert-for-hubverse}
sampled_forecasts_hub <- sampled_forecasts |> 
  rename(value = .sim,
         model_id = .model,
         target_epiweek = epiweek) |> 
  mutate(output_type = "sample",
         output_type_id = stringr::str_c(region, stringr::str_pad(.rep, width = 3, pad = "0")),
         forecast_date = target_epiweek - h*7L) |> 
  ungroup() |> 
  as_tibble() |> 
  select(model_id,
         forecast_date, 
         target_epiweek,
         h,
         output_type,
         output_type_id,
         value)
sampled_forecasts_hub
```

Now we convert these `sample` forecasts to `quantile` forecasts.

```{r convert-to-quantile}
quantiles_to_save <- c(0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975)
quantile_forecasts <- sampled_forecasts_hub |>
  hubUtils::convert_output_type(
    to = list(quantile = quantiles_to_save)
    )
quantile_forecasts
```

::: {.callout-tip collapse="true"}
#### What is happening here?

-   Internally `hubUtils` is calculating the quantiles of the sample-based forecasts.
-   It does this by using a set of default quantiles but different ones can be specified by the user to override the default.
-   It then calls the `quantile()` function from base R to calculate the quantiles.
-   This is estimating the value that corresponds to each given quantile level by ordering the samples and then taking the value at the appropriate position.
:::

### Simple unweighted ensembles

A good place to start when building ensembles is to take the mean or median of the unweighted forecast at each quantile level, and treat these as quantiles of the ensemble predictive distribution.
Typically, the median is preferred when outlier forecasts are likely to be present as it is less sensitive to these.
However, the mean is preferred when forecasters have more faith in models that diverge from the median performance and want to represent this in the ensemble.

::: {.callout-note collapse="true"}
#### Vincent average

The procedure of calculating quantiles of a new distribution as a weighted average of quantiles of constituent distributions (e.g., different measurements) is called a *Vincent average*, after the biologist Stella Vincent who described this as early as 1912 when studying the function of whiskers in the behaviour of white rats.
:::

#### Construction

We can calculate the mean quantile ensemble by taking the mean of the forecasts at each quantile level.
This function could be coded up in a few lines using `dplyr`-style functions, but it is also already built for us by the hubverse, so we will use the `hubEnsembles::simple_ensemble()` function for this:

```{r mean-ensemble}
mean_ensemble <- quantile_forecasts |>
  hubEnsembles::simple_ensemble(
    model_id = "mean_ensemble",
    agg_fun = mean ## this is the default, but writing it to be explicit
  )
```

Similarly, we can calculate the median ensemble by taking the median of the forecasts at each quantile level.

```{r median-ensemble}
median_ensemble <- quantile_forecasts |>
  hubEnsembles::simple_ensemble(
    model_id = "median_ensemble",
    agg_fun = median ## this is the default, but writing it to be explicit
  )
```

We combine the ensembles into a single data frame along with the individual forecasts in order to make visualisation easier.

```{r combine-ensembles}
all_quantile_forecasts <- bind_rows(
  mean_ensemble,
  median_ensemble,
  quantile_forecasts
)
```

#### Visualisation

How do these ensembles visually differ from the individual models?
Lets start by plotting a single forecast from each model and comparing them.
Here we are using the `hubVis` package which creates (by default) an interactive figure using `plotly`. 

```{r plot-single-forecasts}
hubVis::plot_step_ahead_model_output(
  model_out_tbl = all_quantile_forecasts,
  target_data = flu_data |> 
    rename(observation = wili) |> 
    filter(epiweek >= as.Date("2017-09-01")),
  use_median_as_point = TRUE,
  x_col_name = "target_epiweek",
  x_target_col_name = "epiweek",
  facet = "forecast_date",
  facet_nrow = 3,
  intervals = 0.8,
  fill_transparency = 0.5,
  interactive = TRUE
)
```

::: callout-tip
##### Take 5 minutes

How do these ensembles differ from each other?
(Hint: the plot above is interactive, so you can zoom in/out and click on one model in the legend to select/de-select a model for targeted comparison.)
:::

::: {.callout-note collapse="true"}
##### Solution

The ensembles are mostly very similar to each other, except for during the peak of the season, when the `fourier` model is predicting something very different from the other forecasts.
In those cases, the mean ensemble is impacted more, since that single outlying forecast pulls the mean down at various quantile levels.
However, the median ensemble remains close to the three other models that have close agreement, at least for the first few horizons.
:::

#### Evaluation

As in the [forecast evaluation session](forecast-evaluation), we can evaluate the accuracy of the ensembles.
Here, instead of using the `fable` utilities, we will start to use the hubverse `hubEvals` package which relies on the `scoringutils` package under the hood.

In the hubverse, there is a representation of observations called [`oracle output` data](https://docs.hubverse.io/en/latest/user-guide/target-data.html#oracle-output).
This is a way of representing an observed data point as if it were a prediction from an oracle model, that is, one that knows the future before it happens.
The package `hubEvals` expects observations to be stored in this format, so we will first begin by restructuring the `flu_data` into this format.

```{r make-oracle-output}
target_epiweeks <- unique(all_quantile_forecasts$target_epiweek)

oracle_output <- flu_data |> 
  filter(epiweek %in% target_epiweeks) |> 
  rename(target_epiweek = epiweek,
         oracle_value = wili) |> 
  select(-region)

```

::: callout-note
The **weighted interval score (WIS)** is a proper scoring rule for quantile forecasts that approximates the **Continuous Ranked Probability Score (CRPS)** by considering a weighted sum of multiple prediction intervals.
As the number of intervals increases, the WIS converges to the CRPS, combining sharpness and penalties for over- and under-prediction.[@bracherEvaluatingEpidemicForecasts2021]

We see it here as we are scoring quantiles and known distributions hence we cannot use CRPS as we did before.
:::

Again we start with a high level overview of the scores by model.


```{r score-overview}
forecast_scores <- hubEvals::score_model_out(
  all_quantile_forecasts,
  oracle_output, 
  by = "model_id"
)
forecast_scores|> 
  arrange(wis)
```


::: callout-tip
##### Take 5 minutes

What do you think the scores are telling you?
Which model do you think is best?
What other scoring breakdowns might you want to look at?
:::

::: {.callout-note collapse="true"}
##### Solution

What do you think the scores are telling you?
Which model do you think is best?

-   The `fourier_ar2` model appears to be the best performing ensemble model overall according to WIS, although both ensemble models have less bias.
-   Often ensemble forecasts will be better than all individual models, but as this example shows, this is not always the case. 
It is often not the case when one model is substantially better than the others, so the ensemble gives too much weight to models that aren't that good.

What other scoring breakdowns might you want to look at?

-   There might be variation over forecast dates or horizons between the different ensemble methods
:::

### Unweighted ensembles of filtered models

A simple method that is often used to improve ensemble performance is to prune out models that perform very poorly.
Balancing this can be tricky however as it can be hard to know how much to prune.
The key tradeoff to consider is how much to optimise for which models have performed well in the past (and what your definition of the past is, for example all time or only the last few weeks) versus how much you want to allow for the possibility that these models may not perform well in the future.
There is strong evidence from multiple forecasting challenges that forecast accuracy from a given model can vary widely over time.[ray_comparing_2023] 

#### Construction

As we just saw, the random walk model is performing poorly in comparison to the other models.
We can remove this model from the ensemble and see if this improves the performance of the ensemble.

::: callout-warning
##### Warning

Here we are technically cheating a little as we are using the test data to help select the models to include in the ensemble.
In the real world you would not do this as you would not have access to the test data and so this is an idealised scenario.
:::

```{r remove-rw}
filtered_forecasts <- quantile_forecasts |>
  filter(model_id != "rw")
```

We then need to recalculate the ensembles.
First the mean ensemble,

```{r recalculate-ensembles}
filtered_mean_ensembles <- filtered_forecasts |>
  hubEnsembles::simple_ensemble(
    model_id = "mean_ensemble_filtered",
    agg_fun = mean ## this is the default, but writing it to be explicit
  )
```

and then the median ensemble.

```{r recalculate-median-ensemble}
filtered_median_ensembles <- filtered_forecasts |>
  hubEnsembles::simple_ensemble(
    model_id = "median_ensemble_filtered",
    agg_fun = median ## this is the default, but writing it to be explicit
  )
```

We combine these new ensembles with our previous ensembles in order to make visualisation easier.

```{r combine-filtered-ensembles}
filtered_ensembles <- bind_rows(
  filtered_mean_ensembles,
  filtered_median_ensembles,
  all_quantile_forecasts
)
```

#### Visualisation

As for the simple ensembles, we can plot a single forecast from each model and ensemble.

```{r plot-single-filtered-forecasts}
hubVis::plot_step_ahead_model_output(
  model_out_tbl = filtered_ensembles,
  target_data = flu_data |> 
    rename(observation = wili) |> 
    filter(epiweek >= as.Date("2017-09-01")),
  use_median_as_point = TRUE,
  x_col_name = "target_epiweek",
  x_target_col_name = "epiweek",
  facet = "forecast_date",
  facet_nrow = 3,
  intervals = 0.8,
  fill_transparency = 0.5,
  interactive = TRUE
)
```


::: callout-tip
##### Take 2 minutes

Use the interactive plot above to answer these questions.
How do the mean and median filtered ensembles compare to their simple ensemble counterparts?
Which of the two filtered ensembles do you think is better?
:::

::: {.callout-note collapse="true"}
##### Solution

How do the filtered ensembles compare to the simple ensembles?

-   The filtered ensembles appear to be less variable and closer to the eventual observations than the simple ensembles, especially during times of rapid change.

Which do you think is best?

-   Visually, the filtered ensembles appear very similar. 
This makes sense given we know there are only three models left in the ensemble.
There are a few places where it appears the median is a bit closer to the observed data.
:::

#### Evaluation

Let us score the filtered ensembles to obtain a high level overview of the scores by model.

```{r score-filtered-ensembles}
filtered_forecast_scores <- hubEvals::score_model_out(
  filtered_ensembles,
  oracle_output, 
  by = "model_id"
)
filtered_forecast_scores |> 
  arrange(wis)
```

::: callout-tip
##### Take 2 minutes

How do the filtered ensembles compare to the simple ensembles?
:::

::: {.callout-note collapse="true"}
##### Solution

How do the filtered ensembles compare to the simple ensembles?

-   The filtered ensembles appear to be more accurate than the simple ensembles.
-   The ensembles still do not quite beat out the `fourier_ar2` model, although the gap has narrowed. 
-   In general, the median ensemble improved more than the mean ensemble after the filtering.
:::

### Weighted ensembles

The simple mean and median we used to average quantiles earlier treats every model as the same.
We could try to improve performance by replacing this with a weighted mean (or weighted median), for example giving greater weight to models that have made better forecasts in the past.
Here we will explore two common weighting methods based on quantile averaging:

-   Inverse WIS weighting
-   Quantile regression averaging

Inverse WIS weighting is a simple method that weights the forecasts by the inverse of their WIS over some period.
(Note that identifying what this period should be in order to produce the best forecasts is not straightforward as predictive performance may vary over time if models are good at different things).
The main benefit of WIS weighting over other methods is that it is simple to understand and implement.
However, it does not optimise the weights directly to produce the best forecasts.
It relies on the hope that giving more weight to better performing models yields a better ensemble

Quantile regression averaging (QRA), on the other hand, optimises the weights directly in order to yield the best scores on past data.

:::: {.callout-note collapse="true"}
#### Does weighting improve ensemble forecasts?

There is a rich literature on weighting individual forecasts to improve an ensemble.
In epidemiological forecasting, the track record is mixed.
In settings where a long track record (multiple seasons) of performance can be measured and where the accuracy of individual models doesn't vary too much, then weighting has been shown to help with influenza[@reich_accuracy_2019], dengue fever[@colon-gonzalez_probabilistic_2021], and some COVID-19[@ray_comparing_2023] forecasts.
However, in forecasting COVID-19 cases and hospitalizations, where model performance fluctuated over time, it was harder to see improvements in weighted ensembles.[@ray_comparing_2023]

However, estimating weights for a model adds parameters and uncertainty to the ensemble, and there are theoretical reasons to prefer a simple ensemble approach.[@clemen_combining_1989,@claeskens_forecast_2016] 
::::

<!-- #### Construction -->

<!-- ##### Inverse WIS weighting -->

<!-- In order to perform inverse WIS weighting we first need to calculate the WIS for each model. -->
<!-- We already have this from the previous evaluation so we can reuse this. -->

<!-- ```{r calc-wis} -->
<!-- model_scores <- quantile_forecasts |> -->
<!--   score() -->
<!-- cumulative_scores <- list() -->
<!-- ## filter for scores up to the origin day of the forecast -->
<!-- for (day_by in unique(model_scores$origin_day)) { -->
<!--   cumulative_scores[[as.character(day_by)]] <- model_scores |> -->
<!--     filter(origin_day < day_by) |> -->
<!--     summarise_scores(by = "model") |> -->
<!--     mutate(day_by = day_by) -->
<!-- } -->
<!-- weights_per_model <- bind_rows(cumulative_scores) |> -->
<!--   select(model, day_by, wis) |> -->
<!--   mutate(inv_wis = 1 / wis) |> -->
<!--   mutate( -->
<!--     inv_wis_total_by_date = sum(inv_wis), .by = day_by -->
<!--   ) |> -->
<!--   mutate(weight = inv_wis / inv_wis_total_by_date) |> ## normalise -->
<!--   select(model, origin_day = day_by, weight) -->

<!-- weights_per_model |> -->
<!--   pivot_wider(names_from = model, values_from = weight) -->
<!-- ``` -->

<!-- Now lets apply the weights to the forecast models. -->
<!-- As we can only use information that was available at the time of the forecast to perform the weighting, we use weights from two weeks prior to the forecast date to inform each ensemble. -->

<!-- ```{r apply-weights} -->
<!-- inverse_wis_ensemble <- quantile_forecasts |> -->
<!--   as_tibble() |> -->
<!--   left_join( -->
<!--     weights_per_model |> -->
<!--       mutate(origin_day = origin_day + 14), -->
<!--     by = c("model", "origin_day") -->
<!--   ) |> -->
<!--   # assign equal weights if no weights are available -->
<!--   mutate(weight = ifelse(is.na(weight), 1/3, weight)) |> -->
<!--   summarise( -->
<!--     predicted = sum(predicted * weight), -->
<!--     observed = unique(observed), -->
<!--     model = "Inverse WIS ensemble", -->
<!--     .by = c(origin_day, horizon, quantile_level, day) -->
<!--   ) -->
<!-- ``` -->

<!-- ##### Quantile regression averaging -->

<!-- We futher to perform quantile regression averaging (QRA) for each forecast date. -->
<!-- Again we need to consider how many previous forecasts we wish to use to inform each ensemble forecast. -->
<!-- Here we decide to use up to 3 weeks of previous forecasts to inform each QRA ensemble. -->
<!-- We use the `qrensemble` package to perform this task. -->

<!-- ```{r qra} -->
<!-- forecast_dates <- quantile_forecasts |> -->
<!--   as_tibble() |> -->
<!--   pull(origin_day) |> -->
<!--   unique() -->

<!-- qra_by_forecast <- function( -->
<!--   quantile_forecasts, -->
<!--   forecast_dates, -->
<!--   group = c("target_end_date"),  -->
<!--   ... -->
<!-- ) { -->
<!--   lapply(forecast_dates, \(x) { -->
<!--     quantile_forecasts |> -->
<!--       mutate(target_end_date = x) |> -->
<!--       dplyr::filter(origin_day <= x) |> -->
<!--       dplyr::filter(origin_day >= x - (3 * 7 + 1)) |> -->
<!--       dplyr::filter(origin_day == x | day <= x) |> -->
<!--       qra( -->
<!--         group = group, -->
<!--         target = c(origin_day = x), -->
<!--         ... -->
<!--       ) -->
<!--   }) -->
<!-- } -->

<!-- qra_ensembles_obj <- qra_by_forecast( -->
<!--   quantile_forecasts, -->
<!--   forecast_dates[-1], -->
<!--   group = c("target_end_date") -->
<!-- ) -->

<!-- qra_weights <- seq_along(qra_ensembles_obj) |> -->
<!--   lapply(\(i) attr(qra_ensembles_obj[[i]], "weights") |> -->
<!--     mutate(origin_day = forecast_dates[i + 1]) -->
<!--   ) |> -->
<!--   bind_rows() |> -->
<!--   dplyr::filter(quantile == 0.5) |> -->
<!--   select(-quantile) -->

<!-- qra_ensembles <- qra_ensembles_obj |> -->
<!--   bind_rows() -->
<!-- ``` -->

<!-- Instead of creating a single optimised ensemble and using this for all forecast horizons we might also want to consider a separate optimised QRA ensemble for each forecast horizon, reflecting that models might perform differently depending on how far ahead a forecast is produced. -->
<!-- We can do this using `qra()` with the `group` argument. -->

<!-- ```{r qra-by-horizon} -->
<!-- qra_ensembles_by_horizon <- qra_by_forecast( -->
<!--   quantile_forecasts, -->
<!--   forecast_dates[-c(1:2)], -->
<!--   group = c("horizon", "target_end_date"), -->
<!--   model = "QRA by horizon" -->
<!-- ) -->

<!-- qra_weights_by_horizon <- seq_along(qra_ensembles_by_horizon) |> -->
<!--   lapply(\(i) attr(qra_ensembles_by_horizon[[i]], "weights") |> -->
<!--     mutate(origin_day = forecast_dates[i + 2]) -->
<!--   ) |> -->
<!--   bind_rows() -->
<!-- ``` -->


## Sample-based weighted ensembles

Quantile averaging can be interpreted as a combination of different uncertain estimates of a true distribution of a given shape.
Instead, we might want to interpret multiple models as multiple possible versions of this truth, with weights assigned to each of them representing the probability of each one being the true one.
In that case, we want to create a (weighted) mixture distribution of the constituent models.
This can be done from samples, with weights tuned to optimise the CRPS.
The procedure is also called a linear opinion pool.
Once again one can create unweighted, filtered unweighted and weighted ensembles.
For now we will just consider weighted ensembles.
We use the `lopensemble` package to perform this task.

```{r mixture}
## lopensemble expects a "date" column indicating the timing of forecasts
lop_forecasts <- sample_forecasts |>
  rename(date = day)

lop_by_forecast <- function(
  sample_forecasts,
  forecast_dates,
  group = c("target_end_date"),
  ...
) {
  lapply(forecast_dates, \(x) {
    y <- sample_forecasts |>
      mutate(target_end_date = x) |>
      dplyr::filter(origin_day <= x) |>
      dplyr::filter(origin_day >= x - (3 * 7 + 1)) |>
      dplyr::filter(origin_day == x | date <= x)
    lop <- mixture_from_samples(y) |>
      filter(date > x) |>
      mutate(origin_day = x)
    return(lop)
  })
}

lop_ensembles_obj <- lop_by_forecast(
  lop_forecasts,
  forecast_dates[-1]
)

lop_weights <- seq_along(lop_ensembles_obj) |>
  lapply(\(i) attr(lop_ensembles_obj[[i]], "weights") |>
    mutate(origin_day = forecast_dates[i + 1])
  ) |>
  bind_rows()

## combine and generate quantiles from the resulting samples
lop_ensembles <- lop_ensembles_obj |>
  bind_rows() |>
  mutate(model = "Linear Opinion Pool") |>
  rename(day = date) |> ## rename column back for later plotting
  as_forecast_sample() |>
  as_forecast_quantile()

```

## Compare the different ensembles

We have created a number of ensembles which we can now compare.

```{r combine-weigthed-ensembles}
weighted_ensembles <- bind_rows(
  inverse_wis_ensemble,
  qra_ensembles,
  qra_ensembles_by_horizon,
  filtered_ensembles,
  lop_ensembles
) |>
  # remove the repeated filtered ensemble
  filter(model != "Mean filtered ensemble")
```

### Visualisation

#### Single forecasts

Again we start by plotting a single forecast from each model and ensemble.

```{r plot-single-weighted-forecasts}
weighted_ensembles |>
  filter(origin_day == 57) |>
  plot_ensembles(onset_df |> filter(day >= 57, day <= 57 + 14))
```

and on the log scale.

```{r plot-single-weighted-forecasts-log}
weighted_ensembles |>
  filter(origin_day == 57) |>
  plot_ensembles(onset_df |> filter(day >= 57, day <= 57 + 14)) +
  scale_y_log10()
```

### Multiple forecasts

As before we can plot a range of forecasts from each model and ensemble.

```{r plot-multiple-weighted-forecasts}
plot_multiple_weighted_forecasts <- weighted_ensembles |>
  plot_ensembles(onset_df |> filter(day >= 21)) +
  lims(y = c(0, 400))
plot_multiple_weighted_forecasts
```

and on the log scale.

```{r plot-multiple-weighted-forecasts-log}
plot_multiple_weighted_forecasts +
  scale_y_log10()
```

::: callout-tip
##### Take 2 minutes

How do the weighted ensembles compare to the simple ensembles?
Which do you think is best?
Are you surprised by the results?
Can you think of any reasons that would explain them?
:::

::: {.callout-note collapse="true"}
#### Solution

How do the weighted ensembles compare to the simple ensembles?
:::

### Model weights

We can also compare the weights that the different weighted ensembles we have created assign to each model.

```{r plot-model-weights}
weights <- rbind(
  weights_per_model |> mutate(method = "Inverse WIS"),
  qra_weights |> mutate(method = "QRA"),
  lop_weights |> mutate(method = "LOP")
)
weights |>
  ggplot(aes(x = origin_day, y = weight, fill = model)) +
  geom_col(position = "stack") +
  facet_grid(~ method) +
  theme(legend.position = "bottom")
```

::: callout-tip
#### Take 2 minutes

Are the weights assigned to models different between the three methods?
How do the weights change over time?
Are you surprised by the results given what you know about the models performance?
:::

::: {.callout-note collapse="true"}
#### Solution

Are the weights assigned to models different between the three methods?

-   There are major differences especially early on, where the LOP ensemble prefers the random walk model and QRA the more statistical model.
-   The inverse WIS model transitions from fairly even weights early on to giving most weights to the mechanistic model, however it does so in a more balanced manner than the optimised ensemble, giving substantial weight to all three models.

How do the weights change over time?

-   Early on the more statistical models have higher weights in the respective ensemble methods.
-   Gradually the mechanistic model gains weight in both models and by the end of the forecast horizon it represents the entire ensemble.

Are you surprised by the results given what you know about the models performance?

-   As the random walk model is performing poorly, you would expect it to have low weights but actually it often doesn't. This implies that its poor performance is restricted to certain parts of the outbreak.
-   The mechanistic model performs really well overall and dominates the final optimised ensemble.
:::

### Evaluation

For a final evaluation we can look at the scores for each model and ensemble again.
We remove the two weeks of forecasts as these do not have a quantile regression average forecasts as these require training data to estimate.

```{r score-weighted-ensembles}
weighted_ensemble_scores <- weighted_ensembles |>
  filter(origin_day >= 29) |>
  as_forecast_quantile(forecast_unit = c("origin_day", "horizon", "model")) |>
  score()
```

Again we can get a high level overview of the scores by model.

```{r score-overview-weighted}
weighted_ensemble_scores |>
  summarise_scores(by = c("model"))
```

Remembering the session on [forecast evaluation](forecast-evaluation), we should also check performance on the log scale.

```{r score-overview-weighted-log}
log_ensemble_scores <- weighted_ensembles |>
  filter(origin_day >= 29) |>
  as_forecast_quantile(forecast_unit = c("origin_day", "horizon", "model")) |>
    transform_forecasts(
    fun = log_shift,
    offset = 1,
    append = FALSE
  ) |>
  score()

log_ensemble_scores |>
  summarise_scores(by = c("model"))
```

::: callout-tip
#### Take 2 minutes

How do the weighted ensembles compare to the simple ensembles on the natural and log scale?
:::

::: {.callout-note collapse="true"}
#### Solution

The best ensembles slightly outperform some of the simple ensembles but there is no obvious benefit from using weighted ensembles.
Why might this be the case?
:::

# Going further

## Challenge

-   Throughout the course we've been carefully building models from our understanding of the underlying data generating process. What might be the implications of combining models from different modellers, with different assumptions about this process? Think about the reliability and validity of the resulting ensemble forecast.
-   Would it change how you approach communicating the forecast?

## Methods in the real world

-   @howertonContextdependentRepresentationBetweenmodel2023 suggests that the choice of an ensemble method should be informed by an assumption about how to represent uncertainty between models: whether differences between component models is "noisy" variation around a single underlying distribution, or represents structural uncertainty about the system.
-   @sherrattPredictivePerformanceMultimodel2023 investigates the performance of different ensembles in the European COVID-19 Forecast Hub.
-   @amaralPostprocessingWeightedCombination2025 discusses the challenges in improving on the predictive performance of simpler approaches using weighted ensembles.

# Wrap up

-   Review what you've learned in this session with the [learning objectives](../reference/learning_objectives)
-   Share your [questions and thoughts](../reference/help)

## References

::: {#refs}
:::

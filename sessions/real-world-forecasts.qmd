---
title: "Evaluating real-world outbreak forecasts"
order: 12
bibliography: ../nfidd.bib
---

# Introduction

So far in this course we have focused on building, visualising, evaluating, and combining "toy" forecast models in somewhat synthetic settings.
In this session you will work with real forecasts from an existing modeling hub.
This will expose many of the challenges involved with real-time forecasting, as well as the benefits of coordinated modeling efforts.

## Slides

-   [Real-world forecasts](slides/real-world-forecasts)

## Objectives

The aim of this session is to develop advanced skills in working with real forecasts and an appreciation for the challenges of real-time forecasting.

:::: {.callout-note collapse="true"}
# Setup

## Source file

The source file of this session is located at `sessions/real-world-forecasts.qmd`.

## Libraries used

In this session we will use the `nfidd` package to access some stored datasets, the `dplyr` package for data wrangling, the `ggplot2` library for plotting, and the following [hubverse](https://hubverse.io/) packages: `hubData`, hubUtils`, `hubEvals` and `hubEnsembles`.

```{r libraries, message = FALSE}
library("nfidd")
library("dplyr")
library("ggplot2")
library("hubData")
library("hubUtils")
library("hubEvals")
library("hubEnsembles")
theme_set(theme_bw())
```

::: callout-tip
The best way to interact with the material is via the [Visual Editor](https://docs.posit.co/ide/user/ide/guide/documents/visual-editor.html) of RStudio.
:::

## Initialisation

We set a random seed for reproducibility.
Setting this ensures that you should get exactly the same results on your computer as we do.
This is not strictly necessary but will help us talk about the models.

```{r}
set.seed(5050) # for Shohei Ohtani!
```
::::

# Introduction to the US COVID-19 Forecast Hub

The US COVID-19 Forecast Hub ran from 

::: callout-tip
## Some detail about the hub research

:::

## Forecast dimensions

 - weeks
 - locations
 - output-types
 - targets


# The forecasts

We will start by accessing forecast data from the COVID-19 Forecast Hub. 
Both the forecast and target data are stored in an S3 bucket that can be accessed using `hubData`.

::: callout-tip
For a complete list of hubs (many of which have public data available) check out the hubverse [list of hubs](https://hubverse.io/community/hubs.html).
:::


## Accessing forecast data from the cloud

```{r load-forecasts, eval=FALSE}
hub_path_cloud <- hubData::s3_bucket("s3://covid19-forecast-hub")
hub_con_cloud <- hubData::connect_hub(hub_path_cloud, skip_checks = TRUE)

covid_forecasts <- hub_con_cloud |> 
  dplyr::filter(
    output_type == "quantile",
    target == "wk inc covid hosp"
    ) |> 
  dplyr::collect()
```

```{r}
data(covid_forecasts)
```


## Accessing target data from the cloud

Here is code to query the time-series data from the cloud-based hub.
```{r load-ts-target-data, eval = FALSE}
covid_time_series <- connect_target_timeseries(hub_path_cloud) |> 
  filter(target == "wk inc covid hosp") |> 
  dplyr::collect()
```

However, to ensure reproducibility (since this is a live dataset), we have downloaded this object for you already as of July 9, 2025, and it is available to load in from the course R package directly.
```{r load-ts-target-data-archived}
data(covid_time_series)
```



## Exploratory data analysis and visualization

Now that we've downloaded the forecast and target data, let's just do a few basic explorations to make sure we understand the dimensions of our data.
Ideally, the hubverse should ensure that the data is relatively "clean", no typos in targets, no missing quantile levels, no negative predictions, etc...
But let's start by just getting a sense of how many unique values we have for `location`, `horizon`, `target_end_date`, etc...

```{r eda-unique-vals}
unique_per_column <- covid_forecasts  |> 
  select(-value) |> 
  purrr::map(~ sort(unique(.x)))
unique_per_column
```

From the above, we can see that

-   the forecasts were made every week from `r min(unique_per_column$reference_date)` until `r max(unique_per_column$reference_date)`,
-   there is just one target and we only have quantile values for it,
-   we have forecasts for horizons -1 to 3, 
-   there are 23 quantile levels present in the data,
-   there are 17 models,
-   the location codes are numbers.

Before we go any further, let's fix the location codes to be state abbreviations in both the forecast and time-series data.

```{r state-abbrs}
data(covid_locations)
covid_forecasts <- covid_forecasts |> 
  left_join(covid_locations)
covid_time_series <- covid_time_series |> 
  left_join(covid_locations)
```


Let's just do a quick visualization of forecasts from one week to make sure we understand the structure of what one date and location's worth of forecasts look like.

```{r forecast-viz-sanity-check}
covid_forecasts |> 
  filter(reference_date == "2025-02-15", abbreviation == "GA") |> 
  hubVis::plot_step_ahead_model_output(
    target_data = covid_time_series |> 
      filter(as_of == as.Date("2025-07-09"),
             abbreviation == "GA"),
    use_median_as_point = TRUE,
    x_col_name = "target_end_date",
    x_target_col_name = "date",
    pal_color = "Set3"
  )
```


One thing that can foul up evaluation and ensembles of models is when not all models submit at the same times.
Do all models have the same number of predictions? 
Let's look at this a few different ways.

Here is a tabular summary showing some summary stats about how often and how much each model submitted:
```{r model-submission-summary}
covid_forecasts |> 
  group_by(model_id) |> 
  summarise(
    n_submissions = n_distinct(reference_date),
    n_rows = n(),
    n_horizons = n_distinct(horizon),
    n_locations = n_distinct(location)
  ) |> 
  arrange(-n_submissions) |> 
  knitr::kable()
```

And here is a visual showing more details about submissions each week by model, sorted with the models with the most forecasts at the top:

```{r model-submission-viz}
covid_forecasts |> 
  group_by(model_id, reference_date) |> 
  summarise(
    n_rows = n(),
    n_locations = n_distinct(location)
  ) |> 
  ungroup() |> 
  mutate(model_id = reorder(model_id, n_rows, FUN = sum)) |> 
  ggplot() +
  geom_tile(aes(x = reference_date, y = model_id, fill = n_locations))
```

A common challenge in real-time forecasting competitions is that sometimes one week doesn't submit a model.

::: callout-tip
### Take 2 minutes

What are some reasons you can imagine for a model not being submitted in a given week?
:::

::: {.callout-note collapse="true"}
## Solution

Here are some real examples of why models have not been submitted in a given week:

-   The target dataset is not released (as was the case one week in January 2025).
-   The modeler who runs the model every week is sick or on vacation.
-   A model may have joined mid-season because it was still in development.
-   A model have have gone offline because a team stopped participating.
-   ... (there are likely other reasons too)
:::

## What are the models?

You can read about each of the models in [the model-metadata folder of the forecast hub](https://github.com/CDCgov/covid19-forecast-hub/tree/main/model-metadata).
Model metadata can also be programmatically accessed using `hubData::load_model_metadata()`.
There are a wide variety of models submitted to this hub.

::: callout-tip
Here are a few models that we will highlight with their provided descriptions before going further:

This is the hub-generated ensemble forecast:

- **CovidHub-ensemble**: "Median-based ensemble of quantile forecasts submissions."

Here are two models that have been explicitly designed as "baselines" or benchmarks:

- **CovidHub-baseline**: Flat baseline model. The most observed value from the target dataset is the median forward projection. Prospective uncertainty is based on the preceding timeseries." (This is kind of like the `rw` model from previous sessions.)
- **CMU-climate_baseline**: "Using data from 2022 onwards, the climatological model uses samples from the 7 weeks centered around the target week and reference week to form the quantiles for the target week, as one might use climate information to form a meteorological forecast. To get more variation at some potential issue of generalization, one can form quantiles after aggregating across geographic values as well as years (after converting to a rate based case count). This model uses a simple average of the geo-specific quantiles and the geo-aggregated quantiles." (This is kind of like our `fourier` model from previous sessions.)

Here are two models built by InsightNet-funded teams:

- **UMass-ar6_pooled**: "AR(6) model after fourth root data transform. AR coefficients are shared across all locations. A separate variance parameter is estimated for each location."
- **MOBS-GLEAM_COVID**: "Metapopulation, age structured SLIR model. ... The GLEAM framework is based on a metapopulation approach in which the US is divided into geographical subpopulations. Human mobility between subpopulations is represented on a network. ... Superimposed on the US population and mobility layers is an compartmental epidemic model that defines the infection and population dynamics."
:::

::: callout-tip
## Take 2 minutes

Browse through some of the other model descriptions. Which ones look interesting to you and why? Discuss with your neighbor.
:::


# Evaluating forecasts

To quantitatively evaluate the forecasts, we will need the "oracle output" data. Here is code to build it from the time-series target data.

```{r build-oracle-output-data}
covid_oracle_output <- covid_time_series |> 
  filter(as_of == as.Date("2025-07-09")) |> 
  select(target,
         location,
         target_end_date = date,
         oracle_value = observation) 
```


## Visual evaluation



## Overall model comparison

We will start with an overall evaluation of the models, using, as we did in earlier sessions, `hubEvals`.

```{r overall-eval}
scores <- score_model_out(
  covid_forecasts, 
  covid_oracle_output
  )
scores |> 
  arrange(wis) |> 
  knitr::kable()
```


::: callout-tip
## Take 2 minutes

Should we trust the results in this table to give us a reliable ranking for all of the forecasts? Why or why not?
:::

::: {.callout-note collapse="true"}
## Solution

Because there are so many missing forecasts, this single summary is not reliable. We will need a more careful comparison to determine relative rankings of models.

:::

In real-world forecasting hubs, it is very common to have missing forecasts, and it probably isn't safe to assume that these forecasts are "missing at random".
For example, some teams might not submit forecasts when their model's algorithm doesn't converge, indicating a challenging moment to predict in the outbreak, when other teams get low scores but are either consciously or not making different decisions about when to submit.

## Additional metrics

Before we dig further into the evaluation, let's introduce a few additional evaluation metrics and ideas.

### Relative scores

Relative skill scores are a way to create "head-to-head" comparisons of models that to some extent are able to adjust for the difficulty of the predictions made by each model. 
Here is a figure showing how the relative scores are calculated, based on aggregated pairwise comparisons between each pair of models.

![Schematic of relative skill score computation.](slides/figures/relative-skill-scores.png)

The interpretation of the relative skill metric is the factor by which the score for a given model is more or less accurate than the average model, adjusting for the difficulty of the forecasts made by that particular model. 
Relative skill scores that are lower than 1 indicate that the model is more accurate on the whole than the average model evaluated.
For example, a 0.9 relative WIS skill for model A suggests that model A was 10% more accurate on average than other models, adjusting for the difficulty level of forecasts made by model A.

This metric (or a version of it) has been used in practice in a number of large-scale forecast evaluation research papers.[@cramer_evaluation_2022;@meakin_comparative_2022;@sherrattPredictivePerformanceMultimodel2023;@wolfframCollaborativeNowcastingCOVID192023]
The general rule of thumb is that it does a reasonable job comparing models where there is never less than, say, 50% overlap in the targets predicted by any pair of models.

Here is a table of relative skill scores for WIS. 
```{r relative-skill}
score_model_out(
  covid_forecasts, 
  covid_oracle_output,
  metrics = "wis",
  relative_metrics = "wis"
  ) |> 
  arrange(wis_relative_skill)
```

Note that the ordering of the `wis` column does not align perfectly with the `wis_relative_skill` column. 
This is due to the "adjustment" of the relative skill based on which forecasts were made by each model.

To make a more fair comparison, let's subset to only include models that have submitted at least 60% of the maximum possible number of predictions for the season. 

```{r relative-skill-on-subset}
## 33 dates, 5 horizons, 53 locations, 60%
threshold_60pct <- 33 * 5 * 53 * 0.6
model_subset <- covid_forecasts |> 
  filter(output_type_id == 0.5) |> 
  group_by(model_id) |> 
  summarize(targets = n()) |> 
  filter(targets > threshold_60pct) |> 
  pull(model_id)

covid_forecasts |> 
  filter(model_id %in% model_subset) |> 
  score_model_out(
    covid_oracle_output,
    metrics = "wis",
    relative_metrics = "wis"
  ) |> 
  arrange(wis_relative_skill)
```

::: callout-tip
## Take 5 minutes

Looking at the relative and absolute WIS scores from the table above, and the number of total forecasts for each model, what are some of your take-aways from the analysis above?
:::

::: {.callout-note collapse="true"}
## Solution

- The ensemble forecast is the most accurate model overall, by kind of large margin, close to 20% in terms of absolute WIS and 10% in terms of relative skill.
- The CovidHub-baseline has basically "average" performance, with a relative WIS score of 0.96. 
- Four individual models have better performance than the baseline and four have worse performance than the baseline.
- The worst-performing model is the `CMU-climate_baseline`. This is designed to be a "seasonal average" model, and covid isn't that seasonal at the moment, so it's not super surprising that this is a bad model.
- Tying back to approaches from the earlier Nowcasting sessions:
  - The `CEPH-Rtrend_covid` model is "A renewal equation method based on Bayesian estimation of Rt from hospitalization data." It wasn't as good as pure statistical and ML approaches, but was better than the baseline.
  - The `MOBS-GLEAM_COVID` model is a mechanistic model (see description above) and was worse than the baseline.

:::

Since the counts at the national level are likely to dominate the absolute scale of average WIS scores, you could make an argument to subset to only the states and evaluate based on that. 
Here is the analysis above, on that subset:

```{r}
covid_forecasts |> 
  filter(model_id %in% model_subset,
         location != "US") |> 
  score_model_out(
    covid_oracle_output,
    metrics = "wis",
    relative_metrics = "wis"
  ) |> 
  arrange(wis_relative_skill)
```
The results above haven't changed much, although the `CMU-TimeSeries` and `OHT_JHU-nbxd` swapped places in the final rankings.

### Log-scale scores

We can also score on the logarithmic scale. 
This can be useful if we are interested in the relative performance of the model at different scales of the data, for example if we are interested in the model’s performance at capturing the exponential growth phase of the epidemic.[@bosse2023scorin] 
In some sense scoring in this way can be an approximation of scoring the effective reproduction number estimates. 
Doing this directly can be difficult as the effective reproduction number is a latent variable and so we cannot directly score it.

To implement the log-scale scoring, we can simply log-transform the observations and forecasted values (with an offset, to account for zeroes in the data).
```{r log-scale scoring}
covid_forecasts |> 
  filter(model_id %in% model_subset) |> 
  mutate(value = log(value+1)) |> 
  score_model_out(
    covid_oracle_output |> 
      mutate(oracle_value = log(oracle_value+1)),
    metrics = "wis",
    relative_metrics = "wis"
  ) |> 
  arrange(wis_relative_skill)
```

Note that this rescaling of the data prior to scoring results in a more substantial change in the evaluation. 
While the ensemble remains the most accurate model, now five models are better than the baseline, and the `CMU-TimeSeries` model is the most accurate individual model (just barely edging out the `UMass-gbqr` model.)

### Prediction interval coverage

The above comparisons focus on WIS and relative WIS, but in our collaborations with federal, state, and local epidemiologists, we have often found that the metric of prediction interval coverage rates is a useful tool for communicating the reliability of a set of forecasts.
Prediction interval coverage measures the fraction of prediction intervals at a given levels that cover the truth.
For example, if a model is well-calibrated, 50% PIs should cover the truth around 50% of the time, 90% PIs should cover the truth 90% of the time, etc...

::: callout-tip
Prediction interval coverage is not a proper score.
For example, to get perfect 90% PI coverage, you could always make 9 out of every 10 of your intervals infinitely wide and the last one infinitely small. 
Et voila!
90% PI coverage achieved.
:::

But, assuming that modelers aren't trying to game the scores, it can generally be a useful and interpretable metric in practice. 
Let's add some PI coverage metrics to our (unscaled) results from before.

```{r}
covid_forecasts |> 
  filter(model_id %in% model_subset) |> 
  score_model_out(
    covid_oracle_output,
    metrics = c("wis", "interval_coverage_50", "interval_coverage_90")
  ) |> 
  arrange(interval_coverage_90)
```

These results are sorted by the 90% PI coverage, but note that lower is not necessarily "better" here.
What is good is having `interval_coverage_90` scores that are close to 0.90.
A few models (`CovidHub-ensemble`, `CMU-TimeSeries` and `CMU-climate_baseline`) all have coverage rates within 5% of the nominal level for both 50% and 90% PIs.

::: callout-tip
Note, this is a nice example of a model (`CMU-climate_baseline`) being well calibrated but not having good accuracy.
:::


## Model comparison by horizon and forecast date

As we discussed in the [session on evaluating forecasts](forecast-evaluation), overall comparisons can miss details about ways that models perform across different forecast dimensions.
Let's run an analysis looking at the performance of model by horizon and then by forecast date.
For now, we will keep the focus on the 10 models that made at least 60% of possible predictions, and we will only evaluate state-level forecasts.

```{r eval-by-horizon}
scores_by_horizon <- covid_forecasts |> 
  filter(model_id %in% model_subset,
         location != "US") |> 
  score_model_out(
    covid_oracle_output,
    metrics = "wis",
    relative_metrics = "wis",
    by = c("model_id", "horizon")
  ) 

p <- ggplot(scores_by_horizon)+
  geom_line(aes(x = horizon, y = wis, color = model_id))
plotly::ggplotly(p)
```

For looking by `reference_date`, we will also plot the relative WIS.

```{r eval-by-reference_date}
scores_by_reference_date <- covid_forecasts |> 
  filter(model_id %in% model_subset,
         location != "US") |> 
  score_model_out(
    covid_oracle_output,
    metrics = "wis",
    relative_metrics = "wis",
    by = c("model_id", "reference_date")
  ) |> 
  tidyr::pivot_longer(cols = c(wis, wis_relative_skill), 
                      names_to = "metric",
                      values_to = "score")

p <- ggplot(scores_by_reference_date)+
  geom_line(aes(x = reference_date, y = score, color = model_id)) +
  facet_grid(metric~., scales = "free_y")
plotly::ggplotly(p)
```

::: callout-tip
## Take 5 minutes

By horizon and reference date, how variable are the models in their accuracy?

:::

::: {.callout-note collapse="true"}
## Solution

In general, the models seem fairly consistent in their performance. 
There are few "crossings" in the by horizon plot, suggesting that by and large models that are accurate at short-term horizons are also accurate at the longer horizons.
There is much more variability by date, although it still seems that with a few exceptions, models aren't jumping around too dramatically in their relative (or absolute level) WIS. 
:::

# Ensembling forecasts

In this section, we will build several new ensemble forecasts and compare these to the `CovidHub-ensemble` model.
Based on the last results from the section above, we saw that there was some fairly consistent performance from a few of the models. 
This is one precondition for weighted ensembles to have a chance at performing well.
Here are the different approaches we will use:
 - `LOP-unweighted-all`: an unweighted linear opinion pool (LOP) of all available forecasts each week.
 - `LOP-unweighted-select`: an unweighted LOP of the most commonly submitted forecasts.
 - `LOP-weighted-select`: a weighted LOP of the most commonly submitted forecasts.
 - `median-weighted-select`: a weighted median ensemble of the most commonly submitted forecasts.
 
Note that the exisiting `CovidHub-ensemble` forecast is basically `median-unweighted-all` in the above nomenclature. 
It would be hard to do any `weighted-all` ensembles since when models are missing a lot of forecasts it is hard to estimate an accurate weight for them.

## Build unweighted LOP ensembles

Let's start with the "easy" part, building the unweighted ensembles.

::: {.callout-tip collapse="true"}
### Gory LOP details
Note that when we build LOPs with quantile based forecasts, the algorithm needs to approximate a full distribution (the tails are not well-defined in a quantile representation).
There are some assumptions made under the hood about how this is approximated. 
For the really gory details, you can check out [the `hubEnsembles` documentation](https://hubverse-org.github.io/hubEnsembles/dev/articles/hubEnsembles.html?q=weight#creating-ensembles-with-linear_pool)
:::

Note that trying to build all of the forecasts in one call to `linear_pool()` exceeded the memory on your instructors laptop, so we will do a `map_dfr()` "loop" to do this one reference_date at a time.

```{r build-unweighted-ensembles}
# Get a list of unique reference dates
reference_dates <- unique(covid_forecasts$reference_date)

# Map over each date and apply linear_pool separately
lop_unweighted_all <- purrr::map_dfr(
  reference_dates, 
  function(date) {
    covid_forecasts |> 
      filter(model_id != "CovidHub-ensemble", 
             reference_date == date) |> 
      hubEnsembles::linear_pool(model_id = "lop_unweighted_all")
})


lop_unweighted_select <- purrr::map_dfr(
  reference_dates, 
  function(date) {
    covid_forecasts |> 
      filter(model_id %in% model_subset,
             model_id != "CovidHub-ensemble", 
             reference_date == date) |> 
      hubEnsembles::linear_pool(model_id = "lop_unweighted_select")
})
```

## Build weighted ensemble

Inverse WIS weighting is a simple method that weights the forecasts by the inverse of their WIS over some period.
Note that identifying what this period should be in order to produce the best forecasts is not straightforward as predictive performance may vary over time if, say, some models are better at making predictions during different parts of an outbreak.
For example, some models might be good at predicting exponential growth at the beginning of an outbreak but not as good at seeing the slowdown.

The main benefit of WIS weighting over other methods is that it is simple to understand and implement.
However, it does not optimise the weights directly to produce the best forecasts.
It relies on the hope that giving more weight to better performing models yields a better ensemble.

We will use the WIS scores that we have computed above to generate weights for our ensembles.
The weights for one `reference_date` will be based on the inverse of the cumulative average WIS achieved by the model up to the week prior to the current `reference_date`.
Note that since our scores are computed based on data available at the end of the season, this is kind of "cheating", since we're using information that would not have been available to us in real time. 
If we were to do this entirely correctly, we would, for each week, compute the scores based on the available data at that time. 

```{r}
## compute the weights
weights_per_model <- scores_by_reference_date |>
  filter(metric == "wis",
         model_id != "CovidHub-ensemble") |> 
  arrange(model_id, reference_date) |> 
  group_by(model_id) |> 
  mutate(
    cumulative_mean_wis = cummean(score), ## cumulative mean WIS including current week
    cumulative_mean_wis_lag1 = lag(cumulative_mean_wis)
    ) |> 
  ungroup() |> 
  mutate(inv_wis = 1 / cumulative_mean_wis_lag1) |>
  group_by(reference_date) |> 
  mutate(inv_wis_total_by_date = sum(inv_wis, na.rm = TRUE)) |>
  ungroup() |> 
  mutate(weight = inv_wis / inv_wis_total_by_date) |> ## normalise
  select(model_id, reference_date, weight)

## assign equal weights for the first week
first_week_idx <- weights_per_model$reference_date == as.Date("2024-11-23")
weights_per_model[first_week_idx, "weight"] <- 1/sum(first_week_idx)

## assign MOBS a zero weight for a week where it had been missing the week prior
mobs_idx <- which(weights_per_model$model_id == "MOBS-GLEAM_COVID" & 
                    weights_per_model$reference_date == as.Date("2024-11-30"))
weights_per_model[mobs_idx, "weight"] <- 0

## build the weighted ensemble
median_weighted_select <- covid_forecasts |> 
  filter(model_id %in% model_subset,
         model_id != "CovidHub-ensemble") |> 
  simple_ensemble(weights = weights_per_model, 
                  agg_fun = median,
                  model_id = "median_weighted_select")

## build unweighted select median ensemble
median_unweighted_select <- covid_forecasts |> 
  filter(model_id %in% model_subset,
         model_id != "CovidHub-ensemble") |> 
  simple_ensemble(agg_fun = median,
                  model_id = "median_unweighted_select")

```


```{r lop-weighted}
lop_weighted_select <- purrr::map_dfr(
  reference_dates, 
  function(date) {
    covid_forecasts |> 
      filter(model_id %in% model_subset,
             model_id != "CovidHub-ensemble", 
             reference_date == date) |> 
      linear_pool(model_id = "lop_weighted_select",
                  weights = weights_per_model)
})
```


## Compare all ensembles

We will compare the performance of all ensembles just at the state level, since the scores by reference date were only computed at the state level.

```{r}
covid_forecasts_w_ensembles <- bind_rows(
  covid_forecasts,
  lop_unweighted_all,
  lop_unweighted_select,
  lop_weighted_select,
  median_weighted_select,
  median_unweighted_select
)

model_subset_ens <- c("CovidHub-ensemble", 
                      "lop_unweighted_all",
                      "lop_unweighted_select",
                      "median_weighted_select",
                      "median_unweighted_select",
                      "lop_weighted_select")

covid_forecasts_w_ensembles |> 
  filter(model_id %in% model_subset_ens,
         location != "US") |> 
  score_model_out(
    covid_oracle_output
  ) |> 
  arrange(wis)
```

Remember, in the above table, the `CovidHub-ensemble` is kind of like `median-unweighted-all` because it uses a median ensemble of all models available in every week without weights.
Overall, the ensembles have fairly similar average WIS values across all dates and states. 
Using a median ensemble of frequently submitting models (both unweighted and weighted) very marginally outperformed the actual `CovidHub-ensemble` forecasts.
It does not seem like weighting adds a lot of value to these forecasts, although perhaps with additional data on model performance the weights could be improved.


We can also look at how the ensemble performance varied over time.
```{r}
ensemble_scores_by_reference_date <- covid_forecasts_w_ensembles |> 
  filter(model_id %in% model_subset_ens,
         location != "US") |> 
  score_model_out(
    covid_oracle_output,
    metrics = "wis",
    by = c("model_id", "reference_date")
  )

p <- ggplot(ensemble_scores_by_reference_date) +
  geom_line(aes(x = reference_date, y = wis, color = model_id))
plotly::ggplotly(p)
```

It is interesting to note how while the LOP ensembles did slightly worse overall, they actually had slightly more accurate forecasts near the peak of the season, especially in the weeks where the scores were the highest. 

# Going further

## Challenge

-   Once you start looking at lots of plots of forecast scores, it is easy to fall into the trap of not looking at the actual forecasts. 
Looking at the scores of the ensemble forecasts by date, it looks like there was one week when the `CovidHub-ensemble` did much worse than other forecasts. 
Investigate this week in particular. 
What did the forecasts look like that week and what was anomalous about it?
-   In the evaluation of the real-world models, we subset to only models that had submitted a large fraction of forecasts. 
But this omits some interesting models, including the model from Metaculus, a human-judgment forecast aggretation company, and some models from the CDC Center for Forecasting and Outbreak Analytics (CFA).
Adapt the code above to conduct a targeted and fair evaluation of the forecasts made by those models.
-   Look at the [list of hubverse-style hubs](https://hubverse.io/community/hubs.html). 
Using the code above, try to replicate some of the analyses in this session for a different hub. 
What challenges and obstacles do you run into?

## Methods in practice

-   @ray_comparing_2023 evaluates the performance of ensemble forecasts (including some weighted ensembles) in predicting COVID-19 cases, hospitalization and deaths, both with weighted and unweighted ensembles.


# Wrap up

-   Review what you've learned in this session with the [learning objectives](../reference/learning_objectives)

# References
::: {#refs}
:::
